<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Binary Differential Item Functioning - Comprehensive</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-136957978-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-136957978-1');
</script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}

.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">PsychoPDA</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="DIF_index.html">Differential Item Functioning</a>
</li>
<li>
  <a href="measureDiagnostics_index.html">Measure Diagnostics</a>
</li>
<li>
  <a href="designAnalysis_index.html">Design Analysis</a>
</li>
<li>
  <a href="examples_index.html">Examples</a>
</li>
<li>
  <a href="releaseNotes.html">Release notes</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/lucasjfriesen/jamoviPsychoPDA">View on Github</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Binary Differential Item Functioning - Comprehensive</h1>

</div>


<p>The following is a comprehensive overview of the available options in the BinaryDIF module. For information on getting started, see here:</p>
<ul>
<li>
<a href="DIF_binary_gettingStarted.html">Binary Differential Item Functioning - Getting Started</a>
</li>
</ul>
<div id="dif-analysis-background-and-software-implementation" class="section level1">
<h1>DIF Analysis Background and Software Implementation</h1>
<div id="background" class="section level2">
<h2>Background</h2>
<p>In any discussion of differential item functioning, it is important to note that DIF is not synonymous with bias or fairness. Questions of fairness in tests and measures are directly related to the use of an individual’s score on that test or measure. For example, if men and women from a given nation receive different levels of training in speaking English, whether and how scores on a speaking proficiency test should be used in making immigration decisions is a question of fairness. On the other hand, DIF analysis seeks to find out if men and women from that nation <em>who are equally proficient in spite of differing circumstances</em> have an equal probability of answering a given test item correctly. Thus, DIF analysis is a step removed from external questions of bias and fairness in test decisions, but is instead concerned with the internal functioning of the test and test items. However, should a test or test item be found to exhibit DIF in favour of one group over another, this result may be relevant to any decision making process based on test scores, as well as a discussion of fairness regarding the use of that test.</p>
<p>Logistic regression, the method used in this module, is among the most popular methods for detecting DIF for several reasons. Primarily, because most standardized tests and measures use questions which are dichotomized as being “correct” or “incorrect”, logistic regression is the natural tool in this vast majority of testing contexts: it allows one to easily model binary response data (e.g. Kutner et al. 2004 or McCulloch et al. 2008). Logistic regression can be used to detect both uniform and non-uniform DIF with two or more categorical groups or a continuous random variable. That is, it can be applied to questions of DIF between external variables such as binary-classified gender, an n-ary classification of ethnicity or pizza topping preference, or continuous variables such as height or age. This wide applicability makes logistic regression the most flexible method of detecting DIF to date (see Moses, Miao, &amp; Dorans (2010) and Magis et al (2010) for additional discussion). However, no software has yet been developed which facilitates DIF analyses in a way which is widely accessible to researchers not already familiar with sophisticated programming languages, such as R (CITE ME BABY). In addition to facilitating the process of conducting DIF analysis, the software presented below incorporates features of the post-data design analysis proposed by Gelman &amp; Carlin (2014) which aids in the interpretation of DIF analysis results.</p>
</div>
<div id="calculations" class="section level2">
<h2>Calculations</h2>
<p>The module performs DIF analysis on dichotomously scored items using binary grouping variables and either total-score matching or matching based on a supplied variable. The following explanation of the analysis will use an example where the grouping variable G is a binary classification 0/1, such as Male/Female, and the matching variable is a continuous random variable θ, such as scores on a measure of tendency towards verbal aggression. The matching variable is used to control for expected variation. For example, in a measure of language proficiency it is expected that a given item will be easier for individuals who are more highly trained in the language. Thus, if Males generally receive more training than Females, the effect of this training would confound the examination of whether or not an item exhibits gender-DIF unless we account for this differential effect in our model via G * θ.</p>
<p>Assessing DIF using logistic regression involves fitting two models and comparing the model fit of these models. The first is the baseline model which is fit using only the main effect of the matching variable:</p>
<p>Logit of endorsement = β₀+ β1 * θ + ε,</p>
<p>where beta_0 is a fixed intercept, beta_1 is the marginal effect of theta on the response variable, and epsilon is a random perturbation (assumed normally distributed). Note that the logit of endorsement is defined as:</p>
<p>log(Pr(endorsement)/(1-Pr(endorsement)))</p>
<p>A second model is then fit which contains the main effect of the matching variable, the main effect of the grouping variable, and an interaction between the matching and grouping variables:</p>
<p>Logit of endorsement = β₀ + β₁* θ + β2 * G + β3 * (θ * G) + ε</p>
<p>Logically, if the model which includes the grouping variable fits the observed data more closely than the model which does not account for group differences, then the item would appear to be exhibiting DIF. The statistic most commonly used to quantify the difference in model fit between the baseline and full models is the change in model deviance, which is a measure of the poorness of model fit, ranging between 0 and 1, where a change in deviance of 0 indicates perfect fit. (Hancock, Mueller, &amp; Stapleton, 2010, p 236). In standard industry practice, if the change in model deviance is not statistically significant (at a level specified by the researcher and using either the standard (asymptotic) chi-squared Wald test of joint significance or a likelihood ratio test), then the item is not flagged as exhibiting DIF. If the change in model deviance is significant, the item is flagged as exhibiting DIF.</p>
</div>
<div id="options" class="section level2">
<h2>Options</h2>
<p><img src="DIF/analysisVariables.png  " class="img-responsive" alt=""></p>
<div id="items-type-variables" class="section level3">
<h3>1. Items type: Variables</h3>
<p><code>Items</code> must be a series (vector) of names referencing columns in the data containing exactly two unique values, such as [0, 1]. The data will be converted to integer type prior to analysis using <code>jmvcore::toNumeric()</code>.</p>
</div>
<div id="grouping-variable-type-variable" class="section level3">
<h3>2. Grouping Variable type: Variable</h3>
<p><code>Grouping Variable</code> must be a single name referencing a column in the data containing at least two unique values, such as [0, 1]. The data will be converted to factor type prior to analysis using <code>base:as.factor()</code>.</p>
<p>The value of the first row in the column will be coded as <code>Group A</code>, and all rows containing non-<code>Group A</code> values will be coded as <code>Group B</code>. If more than two unique values are present in the column, all values different from the value of the first row will be collapsed into the same <code>Group B</code>. This restriction will be removed in a future version of the module, but is necessary for several aspects of the current version to perform.</p>
</div>
<div id="matching-variable-type-variable" class="section level3">
<h3>3. Matching Variable type: Variable</h3>
<p><code>Matching Variable</code> is an optional argument. This is the value which is used in the regression equations to account for differences in, for example, proficiency, levels of a latent contstruct, or some other psychosocial factor. If it is left empty, subjects will be matched on the Total Score, calculated internally by the module. This variable must be <code>numeric</code>.</p>
</div>
<div id="anchor-items-type-variables" class="section level3">
<h3>4. Anchor Items type: Variables</h3>
<p><code>Anchor Items</code> is an optional argument. If provided, <code>Matching Variable</code> must be left empty. <code>Anchor Items</code> must be a series (vector) of names referencing columns in the data which are considered to be ‘gold standard’. That is, containing no DIF. These items will be used in calculating the subject Total Score, but are not assessed for DIF. See <code>Purification</code> below for more details.</p>
<p><img src="DIF/analysis.png  " class="img-responsive" alt=""></p>
</div>
<div id="type" class="section level3">
<h3>5. Type</h3>
<pre><code>    options:
    - Uniform DIF
    - Non-Uniform DIF
    - Uniform and Non-Uniform DIF
    </code></pre>
<p><code>Type</code> refers to the type of DIF to be assessed for. The default, <code>Uniform and Non-Uniform DIF</code> will check simultaneously for significant main and interaction effects.</p>
</div>
<div id="flagging-criterion" class="section level3">
<h3>6. Flagging Criterion</h3>
<pre><code>    options:
      - &quot;Wald&quot;
      - &quot;LRT&quot;
      
      </code></pre>
<p><code>Flagging Criterion</code> refers to what test should be used in testing for statistical significance of regression coefficients.</p>
</div>
<div id="group-type" class="section level3">
<h3>7. Group type</h3>
<pre><code>    options:
        Discrete Groups (n = 2)
        Discrete Groups (n &gt; 2)
        Continuous Groups
        </code></pre>
<p><code>Group type</code> specifies how the software should treat the data in the grouping variable. “Discrete Groups (n = 2)”&quot; is the default and will consider the first value in the <code>Grouping Variable</code> column to be the reference group in the logistic regression model. “Discrete Groups (n &gt; 2)”&quot; is to be used when contrasts are to be made between more than two discrete levels. The contrasts must be specified using the <code>Contrast groups</code> argument (see below). “Continuous Groups” is for use when the `Grouping Variable&quot; has not been descritized.</p>
</div>
<div id="contrast-groups-type-character" class="section level3">
<h3>8. Contrast groups type: Character</h3>
<p><code>Contrast groups</code> is somewhat awkward, but necessary for specifying which groups are to be used for levels in the logistic regression modeling process in the case of more than two groups, or if you wish to specify which group is to be the reference group. This string must be entered with each group that is to be used as a contrast seperated by one comma. Any groups not specified will be considered the reference group.</p>
<p>For example, if the <code>Grouping Variable</code> column of the data contains two groups {0, 1}, and you want to specify that ‘1’ be the reference group, enter <code>0</code> in the text box.</p>
<p>If the <code>Grouping Variable</code> column of the data contains four groups {0, 1, 2, 3}, and you want to specify that ‘1’ be the reference group, enter <code>0,2,3</code> in the text box. This tells the software that groups {0,2,3} are each to be treated as seperate contrasts.</p>
<p>If the <code>Grouping Variable</code> column of the data contains four groups {0, 1, 2, 3}, and you want to specify that groups {0, 1} should be collapsed into a single reference group contrasted with {2, 3}, enter <code>2,3</code> in the text box. This tells the software that groups {2,3} are to be treated as seperate contrasts and that {0,1} comprise the reference group.</p>
<p><code>Group Type</code> must take either “Discrete Groups” or “Continuous Groups” as its value. Discrete groups are a grouping variable such as binary gender classifications, or national identity. Continuous Groups are a grouping variable such as age.</p>
</div>
<div id="evaluation-scale" class="section level3">
<h3>9. Evaluation Scale</h3>
<pre><code>    options:
          Zumbo-Thomas
          Jodoin-Gierl</code></pre>
<p><code>Evaluation Scale</code> refers to the classification scheme for DIF effect sizes on the Naeglekirke’s R^2 (Δ R^2) scale.</p>
</div>
<div id="alpha-type-number" class="section level3">
<h3>10. Alpha type: Number</h3>
<p><code>Alpha</code> is the desired Type-I error rate for statistical significance tests.r</p>
</div>
<div id="item-purification" class="section level3">
<h3>11. Item Purification</h3>
<p><code>Item purification</code> can be performed by selecting this option. Note that purification is possible only if the test score is considered as the matching criterion.</p>
<p>Purification works as follows: if at least one item is detected as functioning differently at the first step of the process, then the data set of the next step consists in all items that are currently anchor (DIF free) items, plus the tested item (if necessary). The process stops when either two successive applications of the method yield the same classifications of the items (Clauser and Mazor, 1998), or when nrIter iterations are run without obtaining two successive identical classifications. In the latter case a warning message is printed.</p>
</div>
<div id="number-of-iterations-type-number" class="section level3">
<h3>12. Number of Iterations type: Number</h3>
<p><code>Number of Iterations</code> is the maximum number of purification iterations to be used if <code>Item Purification</code> is selected.</p>
</div>
<div id="p-value-adjustment-method" class="section level3">
<h3>13. P-value Adjustment method</h3>
<pre><code>    options: 
        - bonferroni
        - holm
        - hochberg
        - hommel
        - BH
        - BY
        - none
        </code></pre>
<p><code>P-value Adjustment method</code> is the method of correction for multiple comparisons desired:</p>
<p>The adjustment methods include the Bonferroni correction (“bonferroni”) in which the p-values are multiplied by the number of comparisons. Less conservative corrections are also included by Holm (1979) (“holm”), Hochberg (1988) (“hochberg”), Hommel (1988) (“hommel”), Benjamini &amp; Hochberg (1995) (“BH” or its alias “fdr”), and Benjamini &amp; Yekutieli (2001) (“BY”), respectively. A pass-through option (“none”) is also included. The set of methods are contained in the p.adjust.methods vector for the benefit of methods that need to have the method as an option and pass it on to p.adjust.</p>
<p>The first four methods are designed to give strong control of the family-wise error rate. There seems no reason to use the unmodified Bonferroni correction because it is dominated by Holm’s method, which is also valid under arbitrary assumptions.</p>
<p>Hochberg’s and Hommel’s methods are valid when the hypothesis tests are independent or when they are non-negatively associated (Sarkar, 1998; Sarkar and Chang, 1997). Hommel’s method is more powerful than Hochberg’s, but the difference is usually small and the Hochberg p-values are faster to compute.</p>
<p>The “BH” (aka “fdr”) and “BY” method of Benjamini, Hochberg, and Yekutieli control the false discovery rate, the expected proportion of false discoveries amongst the rejected hypotheses. The false discovery rate is a less stringent condition than the family-wise error rate, so these methods are more powerful than the others.</p>
<p>Note that you can set n larger than length(p) which means the unobserved p-values are assumed to be greater than all the observed p for “bonferroni” and “holm” methods and equal to 1 for the other methods.</p>
</div>
</div>
</div>
<div id="design-analysis" class="section level1">
<h1>Design Analysis</h1>
<div id="background-1" class="section level2">
<h2>Background</h2>
<p>Type-S and Type-M errors are post-data calculations that rely on the observed data and an a priori estimated or hypothesized effect size which is determined via information external to the study at hand, such as literature review and subject-matter expertise (Gelman &amp; Carlin, 2014, p. 643). The Type-S error rate is the probability that the observed estimate of the effect size will have the wrong sign (i.e. - vs. +), if statistically significantly different from zero (Gelman &amp; Carlin, 2014, p. 643). Type-M error, or the exaggeration ratio, is the expectation of the absolute value of the effect size divided by the hypothesized true effect size, if statistically significantly different from zero (Gelman &amp; Carlin, 2014, p. 643). In other words, it is the factor by which the observed effect size might be expected to be exaggerated given the assumption of the hypothesized true effect size and statistical significance. To give the reader an intuition for assessing Type-S and Type-M errors, understanding their intimate relationship with power is helpful. In the simpler case of a classic t-test, when true power is 0 the Type-S error will be 0.5 and Type-M error will be infinite. This means that the testing procedure had an equal probability of observing an effect that was larger than the true effect as it did of observing an effect which was smaller than the true effect. That is, it can tell us nothing about the direction of the true effect relative to our observed effect. Even worse, when the true power to detect an effect is 0, the Type-M error being infinite means that a statistically significant observed effect will be, on average, infinitely larger than the true effect. As the true power to detect an effect increases the Type-S and Type-M errors will decrease.</p>
<p>When the true power is 1 the Type-S error will be 0 and the Type-M error will be 1. A Type-S error of 0 means that there is a 0% probability that the observed effect size is in the opposite direction of the true effect size, and a Type-M error of 1 means that the observed effect size (which is necessarily statistically significant by virtue of power = 1) is precisely equal to the true effect size (under replication). Taken together, this means that, as power increases, an observed effect is more and more likely to be an accurate estimate of the unknown true effect. Type-S and Type-M errors have provided a way to quantify the uncertainty associated with using an observed effect as an estimate of the unknown true effect that goes beyond the simple (and relatively uninformative) consequences of statistical power.</p>
<p>In the context of DIF analysis, the effect size most commonly encountered (and that referenced by the Zumbo-Thomas scale) is a derived (meta) statistic: The difference in Nagelkerke’s R² (Δ R²) between the baseline and full models. Because this number is the difference in statistical fit of a logistic model with fewer terms (the baseline) and a logistic model with more terms (the full model), the difference between the two will always be positive. This means that it is not possible to compute a Type-S error rate for this statistic. Regardless, in most contexts, the precise magnitude of the true DIF effect is of great interest, and it is here that the Type-M error rate proves its usefulness. Because the decisions made regarding the fate of an item on the basis of DIF analysis, such as whether or not it is to be used in a test, or whether a test as a whole is admissible for certain uses, depend almost entirely on the criteria used to decide whether an item is exhibiting “too much DIF”, understanding both the uncertainty around the estimated effect size and the nature of the unknown true effect size are vital.</p>
<p>The design analysis is carried out according to the same principles as those laid out in Gelman &amp; Carlin (2014) using three hypothesized true correlations. The hypothesized true correlations correspond to the level thresholds of the zumbo-Thomas DIF evaluation scale: 0 (Null/Negligble DIF), 0.13 (Moderate DIF), 0.26 (Large DIF).</p>
</div>
<div id="calculation" class="section level2">
<h2>Calculation</h2>
<div id="type-m-error" class="section level3">
<h3>Type-M Error</h3>
<p>Calculating the Type-M error rate for Δ R² is completed via a bootstrapping process which generates an empirical distribution for the Δ R² for each item. The R code for this computation is available in Appendix B. The design analysis is then carried out according to the same principles as those laid out in Gelman &amp; Carlin (2014) using three hypothesized true correlations. The hypothesized true correlations correspond to the level thresholds of the Zumbo-Thomas DIF evaluation scale: 0 (Null/Negligible DIF), 0.13 (Moderate DIF), 0.26 (Large DIF). Those familiar with Type-M error calculations may immediately spot a potential problem with the Null DIF hypothesized true effect. Type-M error is the ratio of the expectation of the observed effect and the hypothesized true effect; in the Null DIF hypothesis, this is 0. In order to bypass this problem, the Null effect is actually hypothesized as being 0 plus 2 times the standard error of the estimated effect size, which is derived during the process of bootstrapping the Δ R² empirical distribution. In effect, this hypothesis is that the unknown true effect is 0 +/- a certain degree of measurement error.</p>
<pre><code>        retroDesign.nagR2 &lt;- function(hypTrueEff, myBoot, alpha, sigOnly) {
          # A matrix for results
          rdRes &lt;- matrix(0, nrow = 1, ncol = 4)
          # The observed Δ R^2
          rdRes[1, 1] &lt;- myBoot$t0
          # se of empirical distribution
          rdRes[1, 4] &lt;- observedSE &lt;- boot.printSE(myBoot)[[3]]
          
          # Observed R^2 is the observed effect
          D &lt;- myBoot$t0
          
          # Empirical cumulative density function on the bootstrapped data
          qUpper &lt;- ecdf(myBoot$t)
          # Either compute Type-M at the provided alpha or for the minimum alpha required for significance
          if (sigOnly) {
            # Quantile matching the upper 1 - alpha in the emp. dist.
            qUpper &lt;- quantile(qUpper,  1 - (alpha))
          } else {
            # Quantile matching the observed value
            qUpper &lt;- boot.qEmp(qUpper, boot.pEmp(qUpper, D))
          }
          
          # shifts the empirical distribution by the difference between the observed effect size and the hypothesized true effect size
          myBoot.Shifted &lt;- ecdf(myBoot$t + hypTrueEff)
          # Empirical observed power
          rdRes[1, 3] &lt;- power &lt;- 1 - myBoot.Shifted(qUpper)
          # typeM error rate via Estimation
          estimate &lt;-
            D + sample(myBoot$t, replace = T, size = 10000)
          significant &lt;- estimate &gt; qUpper
          rdRes[1, 2] &lt;-
            typeMError &lt;- mean(estimate[significant]) / D
          return(rdRes)
        }</code></pre>
<p>The above function is derived from that provided by Gelman &amp; Carlin (2014) in their appendix. The changes are made in order to accomodate the reality that we cannot assume a normal distribution in this case.</p>
</div>
<div id="type-s-error" class="section level3">
<h3>Type-S Error</h3>
<p>Type-S error is not calculated because it does not align conceptually with the tests being performed in this type of DIF analysis. The Δ R^2 effect size is a meta-statistic, as opposed to a normal measure of effect size like Cohen’s d, and one of the results of this is that Δ R^2 will only be negative in anomalous situations, meaning that there is no meaningful probability of achieving a statistically significant effect in the ‘wrong’ direction.</p>
<p><img src="DIF/designAnalysis.png  " class="img-responsive" alt=""></p>
</div>
</div>
<div id="options-1" class="section level2">
<h2>Options</h2>
<div id="design-analysis-1" class="section level3">
<h3>1. Design Analysis</h3>
<p><code>Design Analysis</code> is a <code>True</code>/<code>False</code> option, defaulted to <code>False</code>. This can be quite computationally intensive. If this option is selected, and then another option is changed, the module will incorporate the change as soon as possible, but several embedded loops mean that there may be a delay before you see the change take effect.</p>
</div>
<div id="constant-alpha" class="section level3">
<h3>2. Constant Alpha</h3>
<p><code>Constant Alpha</code> is a <code>True</code>/<code>False</code> option, defaulted to <code>True</code>. This option refers to whether or not you want to perform a post-data design analysis using the same alpha as in the DIF analysis, or using the minimum alpha required for significance for <em>all</em> items.</p>
</div>
<div id="bootstrap-n-type-number" class="section level3">
<h3>3. Bootstrap N type: Number</h3>
<p><code>Bootstrap N</code> refers to the number of bootstrap replications to be used in creating the empirical distribution for Δ R^2. This is defaulted to <code>1000</code> as a balance between computation time and precision. Lower numbers will allow faster exploration, but will provide less stable estimates.</p>
</div>
<div id="observed-power" class="section level3">
<h3>4. Observed Power</h3>
<p><code>Observed Power</code> is a <code>True</code>/<code>False</code> option, defaulted to <code>False</code>. Toggling this option reveals the column showing the Empirical Observed Power. This is defaulted to <code>False</code> because its interpretation can be confusing, and will not always provide useful information. N.B. The Empirical Observed Power <em>can</em> equal 1 or 0.</p>
</div>
<div id="hypothesized-true-effect-size-type-string" class="section level3">
<h3>5. Hypothesized True Effect Size type: String</h3>
<p><code>Hypothesized True Effect Size</code> is used to perform design analysis using a non-default hypothesized true effect. If left blank, the analysis will be performed at each of the thresholds of the <code>DIF Flag Scale</code> selected above.</p>
</div>
</div>
</div>
<div id="item-response-curves" class="section level1">
<h1>Item Response Curves</h1>
<div id="details" class="section level2">
<h2>Details</h2>
<p><img src="DIF/ICC.png  " class="img-responsive" alt=""></p>
<div id="item-response-curves-based-on-logistic-regression-type-variables" class="section level3">
<h3>1. Item Response Curves based on Logistic Regression type: Variables</h3>
<p><code>Item Respnse Curves</code> is a list (vector) of names referring to columns in the data selected for DIF analysis for which logistic regression Item Response Curves should be produced.</p>
</div>
</div>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<div id="procedure-notes" class="section level2">
<h2>Procedure Notes</h2>
<p>The descriptive overview provides a textual summary of the whole analysis, including what options were chosen, and documenting any warnings or errors that were encountered during the model fitting process.</p>
</div>
<div id="dif-ananlysis-table" class="section level2">
<h2>DIF Ananlysis Table</h2>
<p>The DIF analysis table provides an overview of the analysis results. For each item the following information is displayed: DIF classification (Zumbo-Thomas criterion), p-value and chi-squared statistic of either the Wald or Likelihood ratio test, Δ R². The purpose of this table is to provide an easily viewable and shareable summary of results.</p>
</div>
<div id="design-analysis-table" class="section level2">
<h2>Design Analysis Table</h2>
<p>The design analysis table displays a list of all the items analyzed, the observed DIF effect, the bootstrapped standard error of the estimate, and Type-M errors rates calculated for each of the three thresholds in the Zumbo-Thomas scale.</p>
</div>
<div id="item-response-curves-1" class="section level2">
<h2>Item Response Curves</h2>
<p>The item response curve can be plotted for any item being assessed for DIF. It provides a visual representation of the full model used in the DIF analysis. The different groups are colour coded, with 95% confidence intervals represented by gray shading overlaying each line. In these images the Y-axis is the probability of endorsement (0 to 1), and the X-axis is the range of the matching variable used in the logistic models.</p>
</div>
</div>
<div id="examples" class="section level1">
<h1>Examples</h1>
<p>Some worked out examples of analyses carried out with jamovi PsychoPDA are posted here (more to come):</p>
<ul>
<li>
<a href="DIF_binary_example_detailed.html">Binary Differential Item Functioning - Detailed Example</a>
</li>
<li>
<a href="DIF_binary_example.html">Binary Differential Item Functioning - Toy Example</a>
</li>
</ul>
<h1>
Comments?
</h1>
<p>
Got comments, issues or spotted a bug? Please open an issue at <a href=" https://github.com/lucasjfriesen/jamoviPsychoPDA/issues "> PsychoPDA</a> on github or <a href="mailto:lucasjfriesen@gmail.com">send me an email</a>
</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
