---
title: "Binary Differential Item Functioning - Comprehensive"
nickname: binaryDIFcomprehensive
topic: binaryDIF
category: DIF
output: 
  html_document:
     includes:
         in_header: ganalytics.txt
     toc: true
     toc_float:
        collapsed: false
---

```{r echo=FALSE,results='hide'}
source("../R/functions.R")
```

The following is a comprehensive overview of the available options in the BinaryDIF module. For information on getting started, see here:

`r list_pages(nickname = "binaryDIFintroduction")`

# DIF Analysis

## Background

In any discussion of differential item functioning, it is important to note that DIF is not synonymous with bias or fairness. Questions of fairness in tests and measures are directly related to the use of an individual’s score on that test or measure. For example, if men and women from a given nation receive different levels of training in speaking English, whether and how scores on a speaking proficiency test should be used in making immigration decisions is a question of fairness. On the other hand, DIF analysis seeks to find out if men and women from that nation _who are equally proficient in spite of unfair circumstances_ have an equal probability of answering a given test item correctly. Thus, DIF analysis is a step removed from external questions of bias and fairness in test decisions, but is instead concerned with the internal functioning of the test and test items. However, should a test or test item be found to exhibit DIF in favour of one group over another, this result may be relevant to any decision making process based on test scores, as well as a discussion of fairness regarding the use of that test.

Logistic regression, the method used in this module, is among the most popular methods for detecting DIF for several reasons (see @mosesComparisonStrategiesEstimating2010 and @magisGeneralFrameworkPackage2010 for additional discussion). Logistic regression can be used to detect both uniform and non-uniform DIF with two or more groups, and can be used in either an IRT or non-IRT context. To date, however, no software has been developed which facilitates DIF analysis in a way which is widely accessible(fn). 

## Calculations

The DIF module performs DIF analysis on dichotomously scored items using binary grouping variables(fn) and either total-score matching or matching based on a supplied variable. The following explanation of the analysis will use an example where the grouping variable β₁ is a binary classification (0, 1) and the matching variable is a latent construct-level variable θ. This example is the case where we are simultaneously assessing for both uniform or non-uniform DIF.

Using a GLIM model, items are fit first with a baseline model containing both the main effects and interaction effect:

Logit of endorsement = β₀ + β₁ + θ + (β₁ * θ)

A second model containing  only the main effect of the matching variable is then fit:

Logit of endorsement = β₀ + θ 

The reduction in model deviance is then calculated between the baseline and secondary models (ΔD). If ΔD is not statistically significant (at a level specifiable by the user and using either the Wald test of joint significance or a likelihood ratio test), then the item is not exhibiting DIF. If ΔD is significant, the item is flagged as exhibiting DIF and the effect size for ΔD may be assessed.

## Options

<img src="DIF/analysisVariables.png  " class="img-responsive" alt="">

1.      title: Items                   type: Variables
2.      title: Grouping Variable       type: Variable
3.      title: Matching Variable       type: Variable
4.      title: Anchor Items            type: Variables

<img src="DIF/analysis.png  " class="img-responsive" alt="">


5.      title: Group Type
    	  options:
        	title: Discrete Groups
        	title: Continuous Groups
6.      title: Evaluation Scale
    	  options:
        	  title: Zumbo-Thomas
        	  title: Jodoin-Gierl

12.      title: Type
    	  options:
        - title: Uniform DIF
        - title: Non-Uniform DIF
        - title: Uniform and Non-Uniform DIF
        
13.     title: Flagging Criterion
    	  options:
    	  - name: "Wald"
    	  - name: "LRT"
    	  
14.     title: Alpha      type: Number

15.     title: Item Purification

16.     title: Number of Iterations      type: Number

17.     title: P-value Adjustment method
    	  options: 
        	- name: bonferroni
        	- name: holm
        	- name: hochberg
        	- name: hommel
        	- name: BH
        	- name: BY
        	- name: none


# Design Analysis

## Background

Type-S and Type-M errors are post-data calculations that rely on the observed data and an estimated “true” effect size which is determined via information external to the study at hand, such as literature review and subject-matter expertise (@gelman2014, p. 643). The Type-S error rate is the probability that the observed estimate of the effect size will have the wrong sign (I.e. - vs. +), if statistically significantly different from zero (@gelman2014, p. 643). Type-M error, or the exaggeration ratio, is the expectation of the absolute value of the effect size divided by the hypothesized true effect size, if statistically significantly different from zero (@gelman2014, p. 643). In other words, it is the factor by which the observed effect size might be expected to be exaggerated given the assumption of the provided true effect size. 

To give the reader an sense for assessing Type-S and Type-M errors, understanding their intimate relationship between power is helpful. In the cases where True Power is 0 and 1, Type-S error will be 0.5 and 0, and Type-M error will be infinite and 1, respectively. A Type-S error of 0.5 means that their is a 50% probability that the observed effect size is in the opposite direction of the True effect size, and a Type-M error of 1 means that the observed effect size is precisely what is expected (under replication) of a statistically significant result.

In the context of DIF analysis, the effect size most commonly encountered (and that referenced by the Zumbo-Thomas scale) is a derived statistic - The change in Naeglekirke’s R^2 (Δ R^2) between the baseline and full models. Because this number will always be positive, it is not possible to compute a Type-S error rate for this statistic. However, in most contexts, the precise magnitude of the true DIF effect is of great interest, and it is here that the Type-M error rate is of great interest. This is because decisions made regarding the fate of an item, such as whether or not it is to be used in a test, or whether a test as a whole is admissible for certain uses, depends almost entirely on the criteria used to decide whether an item is exhibiting “too much DIF”.

## Calculation

Calculating the Type-M error rate for a derived statistic is done via a bootstrapping process which generates an empirical distribution for the Δ R^2 for each item. The design analysis is then carried out according to the same principles as those laid out in Gelman & Carlin (2014) using three hypothesized true correlations. The hypothesized true correlations correspond to the level thresholds of the zumbo-Thomas DIF evaluation scale(fn): 0 (Null/Negligble DIF), 0.13 (Moderate DIF), 0.26 (Large DIF).

<img src="DIF/designAnalysis.png  " class="img-responsive" alt="">

## Options

7.      title: Design Analysis

8.      title: Flagged items only

9.      title: Bootstrap N      type: Number

10.     title: Observed Power

11.     title: Hypothesized True Effect Size      type: String

# Item Response Curves

## Details

<img src="DIF/ICC.png  " class="img-responsive" alt="">

18.     title: Item Characteristic Curves      type: Variables

# Examples

Some worked out examples of analyses carried out with jamovi PsychoPDA are posted here (more to come)

`r include_examples("binaryDIF")`

`r issues()`